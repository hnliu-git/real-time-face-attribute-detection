{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c9a98c2-fd30-48c7-b9ee-0a39b8be360d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import random\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from PIL import Image\n",
    "from torch.optim import AdamW\n",
    "from torchvision import models\n",
    "import torch.nn.functional as F\n",
    "from argparse import ArgumentParser\n",
    "from matplotlib import pyplot as plt\n",
    "from torchvision.transforms import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor\n",
    "import face_recognition\n",
    "import cv2\n",
    "\n",
    "attrs = pd.read_csv('data/list_attr_celeba.csv').replace(-1, 0)\n",
    "\n",
    "classes = attrs.columns\n",
    "n_classes = len(classes) - 1\n",
    "id2class = {i:classes[i+1] for i in range(n_classes)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f317246b-21ec-4f68-8fe7-ff2f7e9ac2bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MultiBinMobileNet(pl.LightningModule):\n",
    "\n",
    "  def __init__(self, n_classes):\n",
    "    super().__init__()\n",
    "\n",
    "    self.save_hyperparameters()\n",
    "    self.n_classes = n_classes\n",
    "    mnet = models.mobilenet_v2()\n",
    "    \n",
    "    # the input for the classifier should be two-dimensional, but we will have\n",
    "    # [batch_size, channels, width, height]\n",
    "    # so, let's do the spatial averaging: reduce width and height to 1\n",
    "    self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "    self.base_model = mnet.features\n",
    "    self.fcs = [nn.Sequential(nn.Dropout(p=0.2), nn.Linear(in_features=mnet.last_channel, out_features=2)).cuda() for _ in range(n_classes)]\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.base_model(x)\n",
    "    x = self.pool(x)\n",
    "    # reshape from [batch, channels, 1, 1] to [batch, channels] to put it into classifier\n",
    "    x = torch.flatten(x, 1)\n",
    "\n",
    "    return [fc(x) for fc in self.fcs]\n",
    "\n",
    "  def configure_optimizers(self):\n",
    "    optimizer = AdamW(self.parameters(), lr=1e-3)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode=\"min\", factor=1e-1, patience=2, verbose=True)\n",
    "    return {'optimizer': optimizer, 'lr_scheduler': scheduler, 'monitor': 'val_loss'}\n",
    "    \n",
    "  \n",
    "  def training_step(self, train_batch, batch_idx):\n",
    "    # Img [bsz, w, h, c]\n",
    "    img, attrs = train_batch\n",
    "    output = self.forward(img)\n",
    "    attrs = torch.unbind(attrs, 1)\n",
    "\n",
    "    train_loss = self.get_loss(output, attrs)\n",
    "    return {'loss': train_loss}\n",
    "\n",
    "  def validation_step(self, val_batch, batch_idx):\n",
    "    img, attrs = val_batch\n",
    "    output = self.forward(img)\n",
    "    attrs = torch.unbind(attrs, 1)\n",
    "\n",
    "    val_loss = self.get_loss(output, attrs)\n",
    "    avg_acc, min_acc, max_acc = calculate_metrics(output, attrs)\n",
    "\n",
    "    return {\"val_loss\": val_loss, 'avg_acc': avg_acc, 'min_acc': min_acc, 'max_acc': max_acc}\n",
    "\n",
    "  def validation_epoch_end(self, outputs):\n",
    "      \"\"\"\"\"\"\n",
    "      val_loss = torch.stack([x[\"val_loss\"] for x in outputs]).mean()\n",
    "      avg_acc = sum([x['avg_acc'] for x in outputs]) / len(outputs)\n",
    "      max_acc = max([x['max_acc'] for x in outputs])\n",
    "      min_acc = min([x['min_acc'] for x in outputs])\n",
    "      self.log(\"val_loss\", val_loss, prog_bar=True, logger=True)\n",
    "      self.log(\"avg_acc\", avg_acc, prog_bar=True, logger=True)\n",
    "      self.log(\"max_acc\", max_acc, prog_bar=True, logger=True)\n",
    "      self.log(\"min_acc\", min_acc, prog_bar=True, logger=True)\n",
    "\n",
    "  def get_loss(self, output, truth):\n",
    "    losses = sum([F.cross_entropy(output[i], truth[i].type(torch.LongTensor).cuda()) for i in range(self.n_classes)])\n",
    "    return losses\n",
    "model = MultiBinMobileNet.load_from_checkpoint('fx-epoch=00-val_loss=9.8684387.ckpt').cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e7f4e40-3e56-4d9e-aed2-d450570dacc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# folder = 'img_align_celeba/img_align_celeba'\n",
    "\n",
    "# imgs = [folder + \"/\" + f for f in sorted(folder)]\n",
    "# attrs = pd.read_csv('list_attr_celeba.csv').replace(-1, 0)\n",
    "\n",
    "# classes = attrs.columns\n",
    "# n_classes = len(classes) - 1\n",
    "# id2class = {i:classes[i+1] for i in range(n_classes)}\n",
    "\n",
    "\n",
    "class CelebDataset(Dataset):\n",
    "  def __init__(self,df,image_path,transform=None,mode='train'):\n",
    "    super().__init__()\n",
    "    self.attr=df.drop(['image_id'],axis=1)\n",
    "    self.path=image_path\n",
    "    self.image_id=df['image_id']\n",
    "    self.transform=transform\n",
    "    self.mode=mode\n",
    "  \n",
    "  def __len__(self):\n",
    "    return self.image_id.shape[0]\n",
    "\n",
    "  def __getitem__(self,idx:int):\n",
    "    image_name=self.image_id.iloc[idx]\n",
    "    image=Image.open(os.path.join(folder,image_name))\n",
    "    attributes=np.asarray(self.attr.iloc[idx].T,dtype=np.float32)\n",
    "    if self.transform:\n",
    "      image=self.transform(image)\n",
    "    return image, attributes    \n",
    "\n",
    "\n",
    "# function to visualize dataset\n",
    "def imshow(images,attr,idx:int):\n",
    "    images=images.cpu().numpy().transpose((0,2,3,1))\n",
    "    plt.imshow(images[idx] * std + mean)\n",
    "    labels=attrs.columns.tolist()\n",
    "    labels=labels[1:]\n",
    "    att=attr[idx].numpy()\n",
    "    labels=[label for label,a in list(zip(labels,att)) if a==1]\n",
    "    plt.xlabel(\"\\n\".join(labels))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "\n",
    "train_transform=transforms.Compose([transforms.Resize((224,224)),\n",
    "                  transforms.RandomVerticalFlip(p=0.5),\n",
    "                  transforms.RandomHorizontalFlip(p=0.5),\n",
    "                  transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.2),\n",
    "                  transforms.ToTensor(),\n",
    "                  transforms.Normalize(mean=mean, std=std)])\n",
    "\n",
    "valid_transform=transforms.Compose([transforms.Resize((224,224)),\n",
    "                  transforms.ToTensor(),\n",
    "                  transforms.Normalize(mean=mean, std=std)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d78b074-dbc5-43ad-a8b9-fd5e5650dca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import face_recognition\n",
    "import cv2\n",
    "from datetime import datetime\n",
    "\n",
    "starttime = datetime.now()\n",
    "\n",
    "# Below includes some basic performance tweaks to make things run a lot faster:\n",
    "#   1. Process each video frame at a smaller resolution (though still display it at full resolution)\n",
    "#   2. Only detect faces in every other frame of video.\n",
    "\n",
    "# Get a reference to webcam #0 (the default one)\n",
    "video_capture = cv2.VideoCapture(0)\n",
    "\n",
    "# Initialize some variables\n",
    "# Extent of scaling up the upper bound\n",
    "alpha = 2\n",
    "# Extent of scaling up the other bounds\n",
    "beta = 8\n",
    "# Extent of reducing to speed up the process\n",
    "reduce_level = 2\n",
    "# Bool for processing other frame\n",
    "process_this_frame = True\n",
    "frame_counter = 0\n",
    "\n",
    "while True:\n",
    "    # Grab a single frame of video\n",
    "    ret, frame = video_capture.read()\n",
    "\n",
    "    # Shape of the frame\n",
    "    s_h, s_w, _ = frame.shape\n",
    "\n",
    "    # Resize frame of video to 1/4 size for faster face recognition processing\n",
    "    small_frame = cv2.resize(frame, (0, 0), fx=1/reduce_level, fy=1/reduce_level)\n",
    "\n",
    "    # Convert the image from BGR color (which OpenCV uses) to RGB color (which face_recognition uses)\n",
    "    rgb_small_frame = small_frame[:, :, ::-1]\n",
    "\n",
    "    # Only process every other frame of video to save time\n",
    "    if process_this_frame:\n",
    "        frame_counter += 1\n",
    "        # Find all the faces and face encodings in the current frame of video\n",
    "        face_locations = face_recognition.face_locations(rgb_small_frame)\n",
    "\n",
    "        # Display the results\n",
    "        for top, right, bottom, left in face_locations:\n",
    "            # Scale back up face locations since the frame we detected in was scaled to 1/4 size\n",
    "            top *= reduce_level\n",
    "            right *= reduce_level\n",
    "            bottom *= reduce_level\n",
    "            left *= reduce_level\n",
    "\n",
    "            h = bottom - top\n",
    "            w = right - left\n",
    "\n",
    "            # Add more margin of the box\n",
    "            top = int(max(top - h / alpha, 0))\n",
    "            left = int(max(left - w / beta, 0))\n",
    "            bottom = int(min(bottom + h / beta, s_h - 1))\n",
    "            right = int(min(right + w / beta, s_w - 1))\n",
    "\n",
    "            rgb_frame_prediction = frame[:, :, ::-1]\n",
    "            frame_tensor = valid_transform(Image.fromarray(rgb_frame_prediction)).unsqueeze(0).cuda()\n",
    "            output = model.forward(frame_tensor)\n",
    "            prediction = [torch.argmax(i, dim=1).cpu() for i in output]\n",
    "\n",
    "            prediction_feature = []\n",
    "            for i in range(len(prediction)):\n",
    "                feature_int = int(prediction[i])\n",
    "                if feature_int:\n",
    "                    prediction_feature.append(id2class[i].replace(\"_\", \" \"))\n",
    "\n",
    "            # Draw a box around the face\n",
    "            cv2.rectangle(frame, (left, top), (right, bottom), (0, 0, 255), 2)\n",
    "            # Draw a label with a name below the face\n",
    "            # cv2.rectangle(frame, (left, bottom - 35), (right, bottom), (0, 0, 255), cv2.FILLED)\n",
    "            font = cv2.FONT_HERSHEY_DUPLEX\n",
    "            \n",
    "            for i in range(0, len(prediction_feature), 3):\n",
    "                prediction_feature_str = \", \".join(prediction_feature[i:i+3])\n",
    "                cv2.putText(frame, prediction_feature_str, (left + 6, bottom + i*5+10), font, 0.6, (255, 255, 255), 1)\n",
    "\n",
    "    # process_this_frame = not process_this_frame\n",
    "    process_this_frame = True\n",
    "    \n",
    "    # Display the resulting image\n",
    "    cv2.imshow('Video', frame)\n",
    "\n",
    "    # Hit 'q' on the keyboard to quit!\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release handle to the webcam\n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
